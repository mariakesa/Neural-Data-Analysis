{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch._C'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-67c902cb2967>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/maria/lib/python3.7/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m __all__ += [name for name in dir(_C)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch._C'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import h5py\n",
    "\n",
    "stim_dat=loadmat('data_full.mat')['data']['stim_full'][0][0]\n",
    "beh_dat=loadmat('data_full.mat')['data']['Behavior_full'][0][0]\n",
    "hf = h5py.File('TimeSeries.h5', 'r')\n",
    "neural_dat=np.array(hf['CellResp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, datetime\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if cuda else 'cpu')\n",
    "\n",
    "class SparseAutoencoderL1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SparseAutoencoderL1, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(83205, 2000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2000, 1000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1000, 500),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(500, 2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 500),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(500, 1000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1000, 2000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2000, 83205),\n",
    "            #nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "def sparse_loss(autoencoder, images):\n",
    "    loss = 0\n",
    "    values = images\n",
    "    for i in range(4):\n",
    "        fc_layer = list(autoencoder.encoder.children())[2 * i]\n",
    "        relu = list(autoencoder.encoder.children())[2 * i + 1]\n",
    "        values = relu(fc_layer(values))\n",
    "        loss += torch.mean(torch.abs(values))\n",
    "    for i in range(3):\n",
    "        fc_layer = list(autoencoder.decoder.children())[2 * i]\n",
    "        relu = list(autoencoder.decoder.children())[2 * i + 1]\n",
    "        values = relu(fc_layer(values))\n",
    "        loss += torch.mean(torch.abs(values))\n",
    "    return loss\n",
    "\n",
    "def model_training(autoencoder, train_loader, epoch):\n",
    "    loss_metric = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    autoencoder.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        images = data\n",
    "        images = Variable(images)\n",
    "        images = images.view(images.size(0), -1)\n",
    "        if cuda: images = images.to(device)\n",
    "        outputs = autoencoder(images)\n",
    "        mse_loss = loss_metric(outputs, images)\n",
    "        l1_loss = sparse_loss(autoencoder, images)\n",
    "        loss = mse_loss + SPARSE_REG * l1_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i + 1) % LOG_INTERVAL == 0:\n",
    "            print('Epoch [{}/{}] - Iter[{}/{}], Total loss:{:.4f}, MSE loss:{:.4f}, Sparse loss:{:.4f}'.format(\n",
    "                epoch + 1, EPOCHS, i + 1, len(train_loader.dataset) // BATCH_SIZE, loss.item(), mse_loss.item(), l1_loss.item()\n",
    "            ))\n",
    "\n",
    "def evaluation(autoencoder, test_loader):\n",
    "    total_loss = 0\n",
    "    loss_metric = nn.MSELoss()\n",
    "    autoencoder.eval()\n",
    "    for i, data in enumerate(test_loader):\n",
    "        images = data\n",
    "        images = Variable(images)\n",
    "        images = images.view(images.size(0), -1)\n",
    "        if cuda: images = images.to(device)\n",
    "        outputs = autoencoder(images)\n",
    "        loss = loss_metric(outputs, images)\n",
    "        total_loss += loss * len(images)\n",
    "    avg_loss = total_loss / len(test_loader.dataset)\n",
    "\n",
    "    print('\\nAverage MSE Loss on Test set: {:.4f}'.format(avg_loss))\n",
    "\n",
    "    global BEST_VAL\n",
    "    if TRAIN_SCRATCH and avg_loss < BEST_VAL:\n",
    "        BEST_VAL = avg_loss\n",
    "        #torch.save(autoencoder.state_dict(), './history/sparse_autoencoder_l1.pt')\n",
    "        print('Save Best Model in HISTORY\\n')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    EPOCHS = 10\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 1e-3\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    LOG_INTERVAL = 100\n",
    "    SPARSE_REG = 1e-3\n",
    "    TRAIN_SCRATCH = True        # whether to train a model from scratch\n",
    "    BEST_VAL = float('inf')\n",
    "    \n",
    "    autoencoder = SparseAutoencoderL1()\n",
    "    if cuda: autoencoder.to(device)\n",
    "    x_train,x_test,stim_train,stim_test=train_test_split(neural_dat,stim_dat.flatten(), test_size=0.33, random_state=42)\n",
    "    train_loader = DataLoader(x_train, batch_size=4,\n",
    "                        shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(x_test, batch_size=1,\n",
    "                        shuffle=False, num_workers=4)\n",
    "    if TRAIN_SCRATCH:\n",
    "        # Training autoencoder from scratch\n",
    "        for epoch in range(EPOCHS):\n",
    "            print('started')\n",
    "            starttime = datetime.datetime.now()\n",
    "            model_training(autoencoder, train_loader, epoch)\n",
    "            endtime = datetime.datetime.now()\n",
    "            print(f'Train a epoch in {(endtime - starttime).seconds} seconds')\n",
    "            \n",
    "            # evaluate on test set and save best model\n",
    "            evaluation(autoencoder, test_loader)\n",
    "            #utput=autoencoder(neural_dat.to_device(device))\n",
    "        print('Trainig Complete with best validation loss {:.4f}'.format(BEST_VAL))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
